{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:28:54.927307Z",
     "start_time": "2021-02-03T08:28:52.939178Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:40.887551Z",
     "start_time": "2021-02-03T08:29:40.717826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to regroup all your imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• LeWagon Kaggle Batch Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/lewagon/data-images/blob/master/ML/kaggle-batch-challenge.png?raw=true' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your first Kaggle competition!\n",
    "\n",
    "Your objective is to **submit online an answer** to the open competition [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "- Fortunately, you have already come across the house dataset in the bootcamp!\n",
    "- You will be semi-guided up to a **baseline model**\n",
    "- Only after will you be free to improve & refine your models\n",
    "- We will approach the problem through **pipelines** (the best practice to take!)\n",
    "\n",
    "A word on Kaggle:\n",
    "- Kaggle will rank your submission amongst all participants!\n",
    "- But don't worry, everyone is publicly removed from the leaderboard after 2 months\n",
    "- You can make to 10 submissions per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kaggle\n",
    "\n",
    "üëâ Create an account on Kaggle if you want to participate in the competition. \n",
    "\n",
    "üëâ Join the [House Prices Challenge](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) \n",
    "\n",
    "üëâ Write down your Kaggle `username` the [result spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) (pick the correct batch!)\n",
    "\n",
    "**Your whole class will compete as a group against the team of TAs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already downloaded for you everything from Kaggle into your current notebook folder:\n",
    "- `train.csv` is your (1460 * 81) training set containing `X` and `y`\n",
    "- `test.csv` is your (1459 * 80) testing set without the associated target `y`!\n",
    "- `sample_submission.csv` describing the format required to submit your answer\n",
    "- `data_description.txt` describing all columns\n",
    "\n",
    "Your goal is to predict the `y_pred` missing from your test set and submit it to discover your test_score & ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Load the train dataset in a DataFrame `data` and create your `X` and `y`. Inspect their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:28:55.731767Z",
     "start_time": "2021-02-03T08:28:55.316322Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can use this direct download link if you don't want to create a Kaggle account\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:28:56.210320Z",
     "start_time": "2021-02-03T08:28:56.204853Z"
    }
   },
   "outputs": [],
   "source": [
    "X = ?\n",
    "y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê£ BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial feature overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80 features is too much to deal with one-by-one for a first baseline pipeline! Let's treat them solely based on their `dtype`:\n",
    "\n",
    "‚ùì How many numerical features vs. categorical features do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:28:59.700769Z",
     "start_time": "2021-02-03T08:28:59.692442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Store the Series `feat_categorical_nunique` containing the number of **unique values** for each categorical feature in our training set. How many unique categories are there in total ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:00.579358Z",
     "start_time": "2021-02-03T08:29:00.566973Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:01.356212Z",
     "start_time": "2021-02-03T08:29:01.352760Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î If we were to `OneHotEncode` all categorical features, our feature matrix `X_preproc` would become pretty big and spare, with almost 300 (highly correlated) features for only 1400 observations. Ideally, we should aim at feeding our model with 50-100 features max (üìö Read this [rule of thumb](https://datascience.stackexchange.com/a/11480/98300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 main strategies to reduce the number of categorical features post-preprocessing:\n",
    "- **[Remove](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)** features that bring too little explanation to our model. This may require statistical analysis of feature importance \n",
    "- **[Ordinally encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** (instead of one-hot-encode) categorical features into integers. However this forces a notion of \"order\" (1>2>3...) that can be detrimental if not set properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the **histogram** of number of unique value per categorical feature. Do you see some quick wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:02.706410Z",
     "start_time": "2021-02-03T08:29:02.540040Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As a starter, what about simply removing all features that have **7 unique values or more**, and one-hot-encode every others? Let's keep ordinal encoding and statistical feature selection for the next iteration.\n",
    "\n",
    "‚ùì Store features to OHE in a list `feat_categorical_small` below. How many features will be OHE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:06.573333Z",
     "start_time": "2021-02-03T08:29:06.566047Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorical features to one-hot-encode\n",
    "feat_categorical_small = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below (and clear the cell once it passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:10.601850Z",
     "start_time": "2021-02-03T08:29:08.693622Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('features_overview',\n",
    "    n=len(feat_categorical_small))\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline - V1 minimal baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "‚ùì Let's code the basic preprocessing pipeline described below. Save it under `preproc_baseline`.\n",
    "\n",
    "For categorical features\n",
    "- Simple-Impute with most frequent values\n",
    "- One-Hot-Encode features that have less than 7 unique values to start with\n",
    "- Drop all others features\n",
    "\n",
    "\n",
    "As for numerical features\n",
    "- Simple-Impute with strategy 'mean'\n",
    "- Min-Max Scale \n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>‚ÑπÔ∏è Pro tips</summary>\n",
    "\n",
    "If you are confident, you can try sklearn's shorter syntax `make_pipeline` or `make_column_transformer` instead of the longer syntax `Pipeline` or `ColumnTransformer` if you want to avoid giving names manually to every steps.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:46.327429Z",
     "start_time": "2021-02-03T08:29:46.298030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy code your pipeline first\n",
    "\n",
    "# Code it for real below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the **shape** of your preprocessed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:48.945399Z",
     "start_time": "2021-02-03T08:29:48.899742Z"
    }
   },
   "outputs": [],
   "source": [
    "shape_preproc_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:51.189687Z",
     "start_time": "2021-02-03T08:29:50.059279Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('preproc_baseline',\n",
    "    shape=shape_preproc_baseline)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Pipe a basic Ridge regressor to your `preproc_baseline` and store it to `pipe_baseline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:52.307918Z",
     "start_time": "2021-02-03T08:29:52.248129Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Read the Kaggle [contest evaluation rules](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) \n",
    "- Which performance metric do you need? Is it readily available in sklearn?\n",
    "- Create a scorer using [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and store it into a variable named `rmsle`\n",
    "- Create also the negative score `rmsle_neg` which is best when _maximized_. This will come handy later as `GridSearchCV` requires a score to _maximize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:48:25.203103Z",
     "start_time": "2021-02-03T08:48:25.197636Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì5-fold cross_validate your `pipe_baseline` using this metric to get a first glance at your baseline perf.    \n",
    "Store your mean score as `score_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:56.980617Z",
     "start_time": "2021-02-03T08:29:56.716448Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Predict `y_pred_baseline` from the Kaggle `test.csv` dataset you stored in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:29:58.984163Z",
     "start_time": "2021-02-03T08:29:58.586080Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Finally, store your CSV ready to be submitted as `submission_baseline.csv` in the `data` folder. Read carefully the Kaggle required format and test it below (you don't need to submit this baseline online for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:30:00.911236Z",
     "start_time": "2021-02-03T08:30:00.904484Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:30:06.294037Z",
     "start_time": "2021-02-03T08:30:05.002420Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "tmp = pd.read_csv(\"data/submission_baseline.csv\")\n",
    "result = ChallengeResult('submission_baseline',\n",
    "    score_baseline = score_baseline,\n",
    "    submission_shape = tmp.shape,\n",
    "    submission_columns = list(tmp.columns),\n",
    "    submission_dtypes = str(list(tmp.dtypes)),\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è ITERATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ üéâ Congratulation for having fully pipelined a basline model! You will see now how easier it is to iterate and improve performance üöÄ\n",
    "\n",
    "- Your goal is to improve your prediction and submit it by **16h30 max online**\n",
    "- We suggested you some improvements below\n",
    "- **Pick up your fights** and **incrementally** improve your pipeline as you see fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "- Linear Models: fine-tune regularization ?\n",
    "- **Tree-based ensembles (must try today)**: Probably the best suited for many categorical-features problems\n",
    "- Stacking !\n",
    "- ...\n",
    "\n",
    "**Preprocessing** (once your first ensemble models works)\n",
    "\n",
    "- Ordinal Encoding of categorical features with a hidden notion of order in their values (e.g. \"bad\", \"average\", good\")\n",
    "- Statistical Feature Selection to remove useless features (avoid overfitting and reduce train time)\n",
    "- Predict log(SalePrice) instead?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Iteration (‚ö†Ô∏è come back here after your first Ensemble model)\n",
    "\n",
    "‚è© Collapse me if you don't use me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding (1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the following feature below. Couldn't it be encoded numerically in a wise manner?\n",
    "```\n",
    "ExterQual: Evaluates the quality of the material on the exterior \n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Luckily, the `OrdinalEncoder` and its argument `categories`  allows us to do just that. Check it out below and make sure to understand how ths works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T15:51:51.765333Z",
     "start_time": "2021-02-02T15:51:51.758709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define specific order for features\n",
    "# Note: if you change this order, it will change the output for .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good'] \n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Just some random training data\n",
    "XX = [\n",
    "    ['good', 'dirty'],\n",
    "    ['bad', 'new'],\n",
    "    ['average', 'clean'],\n",
    "]\n",
    "\n",
    "encoder.fit(XX)\n",
    "\n",
    "encoder.transform([\n",
    "        ['bad', \"dirty\"],\n",
    "        ['good', 'new'],\n",
    "        ['bad', 'oooops never seen this label before']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "‚ùì **Your turn**: split your categorical preprocessor into\n",
    "\n",
    "- `preproc_ordinal` to ordinally encode **some features** of your choice (**do a quick iteration first**)\n",
    "- `preproc_nominal` to one hot encode the other ones\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- You won't be able to avoid hard-coding names and ordered values of features! Be tidy!\n",
    "- It's a good practice to sort alphabetically your features to avoid bad surprises\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T16:11:15.997343Z",
     "start_time": "2021-02-02T16:11:15.977031Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Target engineering (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚ùì We are asked to minimize the RMS**L**E. Why don't we transform our target to directly predict its log?\n",
    "- Check-out historgram of your target `y`. Normally distributed variables should be easier to predict with linear models. \n",
    "- Create `y_log` and your new performance metrics\n",
    "- Don't forget at the end to take the exponential of your predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T15:51:52.314746Z",
     "start_time": "2021-02-02T15:51:51.942808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Feature Selection (1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to remove the least interesting features, to limit overfitting and shorten training time.  \n",
    "Choose one of the 3 options below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### (option 1 - recommended) univariate feature selection based on relationship with target `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚ùì We will use sklearn's [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) tools directly in your pipeline.\n",
    "- Add a `SelectPercentile` filter (coupled with `mutual_info_regression` for the statistical test to use) at the end of your `preproc` pipeline.\n",
    "- This will filter-out features that, - taken individually - least explain your target!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### (option 2) multivariate feature selection based their common relationship with target `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚ùì We will use sklearn's [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) tools directly in your pipeline.\n",
    "- Add a `SequentialFeatureSelector` at the end of your `preproc` pipeline.\n",
    "- This will recursively filter-out least important features according to `feature_permutation` importance!  \n",
    "- Probably overkill for such small dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (option 3) Filter based only on the properties of `X` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Use Pearson's correlation combined with a heatmap...\n",
    "- to check visually whether some **numerical** features almost entirely explain others. \n",
    "- Then, create a \"filter\" in your pipeline that removes any correlated below a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T15:51:54.086492Z",
     "start_time": "2021-02-02T15:51:54.084287Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Use [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) combined with a heatmap...\n",
    "- to whether some **ordinally encoded** features are almost entirely \"ordered\" similarily than others\n",
    "- Then, create a \"filter\" in your pipeline that removes any ordinal features correlated below a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T15:51:54.378627Z",
     "start_time": "2021-02-02T15:51:54.376048Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Another way to filter out feature relies solely on removing those with the lowest variance.  \n",
    "- Think about it: a feature which only takes one value is useless (and has a variance of 0).  \n",
    "- Try to add a `VarianceThreshold` to the end of your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T15:51:55.855088Z",
     "start_time": "2021-02-02T15:51:55.853045Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final preproc pipeline\n",
    "‚ùì store here your final version of the preproc pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T19:56:27.306869Z",
     "start_time": "2021-02-02T19:56:27.161247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÖFINAL SUBMISSION (start at 4h30 max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover your real test score by submitting on Kaggle! \n",
    "\n",
    "üëâ Write down your test score on the [result spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) (pick the correct batch!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.861551Z",
     "start_time": "2021-01-27T14:04:03.536Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.997px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
